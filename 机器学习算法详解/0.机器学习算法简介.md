### 一、监督学习算法（基于标签数据训练，用于预测或分类）
#### （一）单一模型（独立训练的基础模型）
##### 1. 线性回归（Linear Regression）
- **任务类型**：回归任务（预测连续数值输出）。  
- **核心原理**：假设输入特征与输出变量存在线性关系，通过最小化预测值与真实值的**平方误差（MSE）** 拟合线性方程 \( y = w^Tx + b \)（\( w \) 为权重，\( b \) 为偏置）。  
- **扩展变体**：  
  - 岭回归（Ridge Regression）：加入L2正则化，缓解过拟合，处理多重共线性；  
  - Lasso回归：加入L1正则化，可实现特征选择（权重稀疏）；  
  - 弹性网络（Elastic Net）：结合L1和L2正则化，平衡特征选择与稳定性。  
- **适用场景**：  
  - 输出为连续值的预测问题（如房价预测、销售额预测、温度预测）；  
  - 特征与输出的关系近似线性，且特征维度较低的场景。  
- **实践要点**：  
  - 需处理**多重共线性**（可通过VIF检验或正则化解决）；  
  - 对异常值敏感，需先清洗数据；  
  - 若关系是非线性，需手动添加多项式特征。  


##### 2. K近邻算法（KNN，K-Nearest Neighbors）
- **任务类型**：分类任务（主流）或回归任务（少数）。  
- **核心原理**：通过计算新样本与训练集中所有样本的距离，选取最近的\( k \)个样本，用这\( k \)个样本的标签（分类时投票，回归时平均）作为预测结果。  
- **扩展变体**：  
  - 加权KNN：距离越近的样本权重越大；  
  - 球树（Ball Tree）/KD树：优化高维数据下的距离计算效率。  
- **适用场景**：  
  - 小数据集场景（计算复杂度随样本量增长而增加）；  
  - 特征维度较低的分类问题（如鸢尾花分类、手写数字识别）；  
  - 可解释性要求不高，但需要“直观”判断的场景（如基于用户行为的推荐）。  
- **实践要点**：  
  - 必须对特征做**归一化/标准化**（避免距离被量级大的特征主导）；  
  - \( k \)值需通过交叉验证选择（\( k \)太小易过拟合，太大易欠拟合）；  
  - 距离度量选择（欧氏距离适用于连续特征，曼哈顿距离适用于高维稀疏特征）。  


##### 3. 朴素贝叶斯（NB，Naive Bayes）
- **任务类型**：分类任务（二分类/多分类）。  
- **核心原理**：基于**贝叶斯定理**（\( P(A|B) = \frac{P(B|A)P(A)}{P(B)} \)），假设特征之间相互独立（“朴素”的核心），通过计算后验概率最大的类别作为预测结果。  
- **扩展变体**：  
  - 高斯朴素贝叶斯（Gaussian NB）：假设特征服从高斯分布，适用于连续特征；  
  - 多项式朴素贝叶斯（Multinomial NB）：适用于离散计数特征（如文本词频）；  
  - 伯努利朴素贝叶斯（Bernoulli NB）：适用于二元特征（如文本是否出现某词）。  
- **适用场景**：  
  - 高维稀疏数据（如文本分类、垃圾邮件识别、情感分析）；  
  - 样本量较大但计算资源有限的场景（训练速度极快）；  
  - 对误判代价不敏感的场景（如新闻分类）。  
- **实践要点**：  
  - 特征独立性假设可能不成立，但实际效果往往较好；  
  - 需处理“零概率问题”（通过拉普拉斯平滑或加1平滑避免某特征组合未出现导致概率为0）；  
  - 对特征缩放不敏感（无需归一化）。  


##### 4. 逻辑回归（LR，Logistic Regression）
- **任务类型**：分类任务（主流为二分类，可扩展至多分类）。  
- **核心原理**：将线性回归的输出（\( w^Tx + b \)）通过**sigmoid函数**（\( \sigma(z) = \frac{1}{1+e^{-z}} \)）映射到[0,1]区间，作为样本属于某一类的概率，通过最小化交叉熵损失拟合参数。  
- **扩展变体**：  
  - 多项逻辑回归（Multinomial LR）：处理多分类（如手写数字10分类）；  
  - 加权逻辑回归：应对类别不平衡（对少数类样本赋予更高权重）。  
- **适用场景**：  
  - 二分类问题（如点击率预测、信用评分、疾病诊断）；  
  - 需输出概率的场景（如风险评估）；  
  - 特征与类别呈线性关系的场景（非线性需手动加特征）。  
- **实践要点**：  
  - 需处理**类别不平衡**（如SMOTE采样、调整类别权重）；  
  - 需添加正则化（L1/L2）防止过拟合；  
  - 可通过特征交叉（如\( x_1 \times x_2 \)）捕捉非线性关系。  


##### 5. 支持向量机（SVM，Support Vector Machine）
- **任务类型**：分类任务（主流），也可扩展为回归（SVR）。  
- **核心原理**：在特征空间中寻找**最大间隔超平面**（使两类样本到超平面的距离最远），通过“支持向量”（距离超平面最近的样本）确定超平面；对于非线性问题，通过**核函数**（如RBF、多项式核）将低维数据映射到高维空间，转化为线性可分问题。  
- **扩展变体**：  
  - 支持向量回归（SVR）：通过“ε-间隔带”拟合连续值；  
  - 线性SVM（Linear SVM）：适用于高维数据（如文本），计算效率高；  
  - 软间隔SVM：允许少量样本越界，通过参数\( C \)控制容错程度。  
- **适用场景**：  
  - 中小规模数据集（训练复杂度随样本量增长较快）；  
  - 非线性分类问题（如图像识别、文本分类）；  
  - 特征维度高但样本量少的场景（如基因数据分类）。  
- **实践要点**：  
  - 需对特征做**归一化**（核函数对特征尺度敏感）；  
  - 核函数选择（线性核适用于高维稀疏数据，RBF核适用于非线性数据，多项式核适用于低维非线性）；  
  - 调参关键（\( C \)：控制正则化强度；\( \gamma \)：RBF核的带宽，影响非线性拟合能力）。  


##### 6. 决策树（DT，Decision Tree）
- **任务类型**：分类任务（如C4.5、ID3）和回归任务（如CART回归树）。  
- **核心原理**：基于特征对样本进行递归分裂（如“年龄>30？”“收入>5万？”），分裂准则为最大化信息增益（分类）或最小化方差（回归），最终形成树形结构，预测时通过路径到达叶节点输出结果。  
- **扩展变体**：  
  - ID3：基于信息增益分裂，仅支持离散特征；  
  - C4.5：ID3的改进，用信息增益率分裂，支持连续特征和剪枝；  
  - CART（分类与回归树）：用Gini系数（分类）或均方误差（MSE，回归）分裂，输出二叉树。  
- **适用场景**：  
  - 可解释性要求高的场景（如医疗诊断、风控规则提取）；  
  - 特征包含离散值或连续值的混合数据；  
  - 无需特征归一化的场景（对缺失值不敏感，可自动处理）。  
- **实践要点**：  
  - 易过拟合（深度过深导致“过拟合”），需通过剪枝（预剪枝/后剪枝）或限制深度优化；  
  - 对噪声敏感，需先清洗数据；  
  - 分裂可能偏向取值多的特征（如ID3），需注意特征选择。  


#### （二）集成模型（通过组合多个基模型提升性能，均为监督学习）
##### 7. 随机森林（RF，Random Forest）
- **任务类型**：分类与回归（基于决策树集成）。  
- **核心原理**：采用**Bagging（bootstrap aggregating）** 策略：通过bootstrap抽样生成多个训练子集，每个子集训练一棵决策树（基模型），预测时通过投票（分类）或平均（回归）输出结果；同时引入“特征随机”（每棵树仅用随机子集特征分裂），增强基模型多样性。  
- **扩展变体**：  
  - 极端随机树（Extra Trees）：分裂时随机选择阈值（而非最优阈值），进一步降低树的相关性，训练更快。  
- **适用场景**：  
  - 结构化数据（表格数据）的分类/回归（如房价预测、客户流失预测）；  
  - 高维数据（如基因数据、用户行为特征）；  
  - 对可解释性要求中等，但需高性能的场景。  
- **实践要点**：  
  - 树的数量（\( n\_estimators \)）：越多性能越稳定，但计算成本增加（通常100-1000）；  
  - 特征采样比例（如分类任务默认\( \sqrt{d} \)，\( d \)为特征数）；  
  - 无需特征归一化，对缺失值不敏感。  


##### 8. GBDT（Gradient Boosting Decision Tree）
- **任务类型**：分类与回归（基于决策树集成）。  
- **核心原理**：采用**梯度提升**策略：迭代训练决策树（基模型为弱树，如深度1-5的CART树），每棵树拟合前序模型的**残差**（预测值与真实值的差距），通过“加法模型”（\( f(x) = f_{t-1}(x) + \alpha h_t(x) \)，\( \alpha \)为学习率）累积结果；残差计算通过梯度下降实现（“梯度提升”的由来）。  
- **扩展变体**：  
  - 梯度提升回归树（GBRT）：用于回归任务；  
  - 多分类GBDT：通过“一对多”或“多输出”策略扩展。  
- **适用场景**：  
  - 结构化数据的高精度预测（如推荐系统、金融风控）；  
  - 特征与输出呈非线性关系的场景；  
  - 对计算资源有一定容忍度的场景（训练速度较慢）。  
- **实践要点**：  
  - 学习率（\( \alpha \)，通常0.01-0.1）与树的数量需平衡（学习率小则需更多树）；  
  - 树的深度（通常3-5）：过深易过拟合；  
  - 对异常值敏感（需先处理异常值）。  


##### 9. XGBoost（Extreme Gradient Boosting）
- **任务类型**：分类与回归（基于梯度提升的高效实现）。  
- **核心原理**：在GBDT基础上优化：加入**正则化项**（控制树的复杂度）、支持**并行分裂**（特征增益计算并行）、处理**缺失值**（自动学习缺失值的分裂方向）、采用二阶泰勒展开加速梯度下降，训练效率和泛化能力优于传统GBDT。  
- **扩展变体**：  
  - XGBoost-Linear：线性模型与树模型的组合；  
  - 分布式XGBoost：支持大规模数据并行训练。  
- **适用场景**：  
  - 机器学习竞赛（如Kaggle）的高维结构化数据；  
  - 对预测精度要求高且计算资源充足的场景（如金融风控、用户增长预测）；  
  - 需处理缺失值或类别特征的场景。  
- **实践要点**：  
  - 核心调参（\( learning\_rate \)：0.01-0.3；\( max\_depth \)：3-8；\( subsample \)：0.6-1.0；\( colsample\_bytree \)：0.6-1.0）；  
  - 类别特征需手动编码（如one-hot或目标编码）；  
  - 防止过拟合（增加正则化参数\( reg\_alpha/reg\_lambda \)）。  


### 二、无监督学习算法（无标签数据，用于发现数据内在结构）
##### 10. K-Means
- **任务类型**：聚类任务（将数据分为\( k \)个簇）。  
- **核心原理**：通过迭代优化实现：1）随机初始化\( k \)个簇中心；2）计算每个样本到簇中心的距离，将样本分配到最近的簇；3）更新簇中心为簇内样本的均值；重复2-3直到簇中心稳定（或迭代次数达标），目标是最小化**簇内平方和（WCSS）**。  
- **扩展变体**：  
  - K-Means++：优化初始簇中心选择（使中心尽可能远离），提高稳定性；  
  - Mini-Batch K-Means：用小批量样本更新簇中心，适用于大规模数据；  
  - 球形K-Means（Spherical K-Means）：适用于文本等单位向量数据。  
- **适用场景**：  
  - 数据探索与分群（如客户分群、用户画像、产品分类）；  
  - 异常检测（簇外样本视为异常）；  
  - 预处理（如特征降维前的聚类）。  
- **实践要点**：  
  - 需手动指定\( k \)值（通过肘法、轮廓系数或业务经验选择）；  
  - 对特征做**归一化**（距离对尺度敏感）；  
  - 对噪声和异常值敏感（可先去除异常值）；  
  - 适用于凸形簇（非凸簇效果差，需用DBSCAN等算法）。  


### 分类总结表
| 学习范式       | 模型类型       | 算法名称               |
|----------------|----------------|------------------------|
| 监督学习       | 单一模型       | 线性回归、KNN、朴素贝叶斯、逻辑回归、SVM、决策树 |
| 监督学习       | 集成模型       | 随机森林、GBDT、XGBoost |
| 无监督学习     | 聚类模型       | K-Means                |